{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"Sentiment Analysis Project for Tweets using Machine Learning\"\n",
    "\n",
    "Written by Beyzagul Demir, Ersel R. Ekmen, H. Alper Karadeniz\n",
    "\n",
    "Supervisor: Assoc. Prof. Reyyan Yeniterzi\n",
    "\"\"\"\n",
    "\n",
    "### PART 1: Scrape tweets from Twitter using Twint ###\n",
    "\n",
    "!pip install twint # Twitter scraping tool allowing us to scrape Tweets from Twitter profiles without using Twitter's API.\n",
    "!pip install --upgrade pip # Upgrade pip if needed.\n",
    "!pip install nest-asyncio # Patch asyncio to allow nested event loops\n",
    "__import__('IPython').embed()\n",
    "\n",
    "!pip install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master #egg=twint\n",
    "\n",
    "!pip install deep_translator \n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "\n",
    "import os\n",
    "if os.path.exists('search_result.csv'):\n",
    "    os.remove('search_result.csv')\n",
    "\n",
    "# Setting all configurations to filter tweets that will be scraped based on tweeting date and keyword:\n",
    "c = twint.Config()\n",
    "c.Search = \"afgan\"\n",
    "c.Since = \"2021-08-18\"\n",
    "c.Until = \"2021-08-19\"\n",
    "c.Store_csv = True # Tweets are listed in a csv file.\n",
    "c.Output = \"search_result.csv\" # Tweets are listed \"search_result.csv\" file as output in computer.\n",
    "\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35196513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Set target language of tweets and filter them based on the number of like and length of tweet:\n",
    "target_language = \"tr\" # Target language of tweets are Turkish for this project.\n",
    "target_min_likes = 100\n",
    "target_min_length = 10\n",
    "\n",
    "def cleanText(text): # Eliminate useless characters which will not be evaluated in sentiment analysis from tweets:\n",
    "  text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "  text = re.sub(r'#', '', text)\n",
    "  text = re.sub(r'RT[\\s]+', '', text)\n",
    "  text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "  text = re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "  return text\n",
    "\n",
    "import csv\n",
    "\n",
    "all_tweets_list = [] # Create a list to contain all scraped tweets.\n",
    "\n",
    "with open('search_result.csv', mode='r', encoding='utf-8') as infile: #Cleaning tweets from useless characters to make them ready to be analyzed:\n",
    "    reader = csv.reader(infile)\n",
    "    \n",
    "    counter = 0\n",
    "    for rows in reader:\n",
    "        tweet_details = rows\n",
    "\n",
    "        clean_tweet = cleanText(tweet_details[10])\n",
    "\n",
    "        lines = clean_tweet.split(\"\\n\")\n",
    "        non_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
    "\n",
    "        string_without_empty_lines = \"\"\n",
    "        for line in non_empty_lines:\n",
    "            string_without_empty_lines += line + \" \"\n",
    "\n",
    "        clean_tweet = re.sub(' +', ' ', string_without_empty_lines)\n",
    "        \n",
    "        tweet_details.append(clean_tweet)\n",
    "        \n",
    "        if counter != 0:\n",
    "            all_tweets_list.append(tweet_details)\n",
    "        counter+=1\n",
    "\n",
    "if os.path.exists('search_result_new.csv'):\n",
    "    os.remove('search_result_new.csv')\n",
    "\n",
    "with open('search_result_new.csv', mode='w', encoding='utf-8', newline='') as outfile: # Write down filtered tweets to a new file with headers:\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow([\"id\", \"conversation_id\", \"created_at\", \"date\", \"time\", \"timezone\", \"user_id\", \"username\", \"name\", \"place\", \"tweet\", \"language\", \"mentions\", \"urls\", \"photos\", \"replies_count\", \"retweets_count\", \"likes_count\", \"hashtags\", \"cashtags\", \"link\", \"retweet\", \"quote_url\", \"video\", \"thumbnail\", \"near\", \"geo\", \"source\", \"user_rt_id\", \"user_rt\", \"retweet_id\", \"reply_to\", \"retweet_date\", \"translate\", \"trans_src\", \"trans_dest\", \"clean_tweet\"])\n",
    "    \n",
    "    for i in range(len(all_tweets_list)):\n",
    "        \n",
    "        tweet = all_tweets_list[i]\n",
    "        \n",
    "        writer.writerow(tweet)\n",
    "\n",
    "if os.path.exists('search_result_filtered.csv'):\n",
    "    os.remove('search_result_filtered.csv')\n",
    "\n",
    "# Get specific data from \"search_result_new.csv\" file\n",
    "# to \"search_result_filtered.csv\" including 6 rows' information from previous file:\n",
    "\n",
    "with open('search_result_filtered.csv', mode='w', encoding='utf-8', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    writer.writerow([\"subjectivity\", \"polarity\", \"replies_count\", \"retweets_count\", \"likes_count\", \"link\", \"clean_tweet\", \"translated_tweet\"])\n",
    "    \n",
    "    for i in range(len(all_tweets_list)):\n",
    "        \n",
    "        tweet = all_tweets_list[i]\n",
    "        \n",
    "        \n",
    "        language = tweet[11]\n",
    "        replies_count = tweet[15]\n",
    "        retweets_count = tweet[16]\n",
    "        likes_count = tweet[17]\n",
    "        link = tweet[20]\n",
    "        clean_tweet = tweet[36]\n",
    "        \n",
    "        if (language == target_language and int(likes_count) >= target_min_likes and len(clean_tweet.split(\" \")) >= target_min_length):\n",
    "            \n",
    "            import deep_translator\n",
    "            from deep_translator import GoogleTranslator\n",
    "\n",
    "            # Translating Turkish tweets to English so as to allow the NLP algorithm to analyze:\n",
    "            translated_tweet = GoogleTranslator(source='auto', target='en').translate(clean_tweet)\n",
    "            \n",
    "            from textblob import TextBlob\n",
    "\n",
    "            # Calculating subjectivity and polarity of translated tweets:\n",
    "            subjectivity = TextBlob(translated_tweet).subjectivity\n",
    "            polarity = TextBlob(translated_tweet).polarity\n",
    "\n",
    "            writer.writerow((subjectivity, polarity, replies_count, retweets_count, likes_count, link, clean_tweet, translated_tweet))\n",
    "\n",
    "print(\"Successfully completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db92df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dbd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('search_result_filtered.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba9a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
